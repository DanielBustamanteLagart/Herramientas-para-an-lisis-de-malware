#Prueba 1 de el análisis de datos sobre malware con Bosques aleatorios
#La data adquirida es de kaggle https://www.kaggle.com/datasets/amauricio/pe-files-malwares/data
#Daniel Bustamante Lagart 


# Importación de Librerías
import numpy as np
import pandas as pd
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import os
import warnings
warnings.simplefilter('ignore')

# Leer el Archivo con los Datos del Malware
df = pd.read_csv('dataset_malwares.csv')

# Traer los Encabezados
#print(df.head(5))

# Imprimir Información del Dataset
#print(df.info())

# Eliminar Columnas no Deseadas
dropped_df = df.drop(['Name', 'Machine', 'TimeDateStamp', 'Malware'], axis=1)

# Visualizar la Distribución de Clases
ax = sns.countplot(df['Malware'])
ax.set_xticks([0, 1])
ax.set_xticklabels(['Not Malware', 'Malware'])
#plt.show()

# Definir Características de Interés
features = ['MajorSubsystemVersion', 'MajorLinkerVersion', 'SizeOfCode', 'SizeOfImage', 'SizeOfHeaders', 'SizeOfInitializedData', 
            'SizeOfUninitializedData', 'SizeOfStackReserve', 'SizeOfHeapReserve', 'NumberOfSymbols', 'SectionMaxChar']
i = 1

# Visualizar Distribuciones de Características
for feature in features:
    plt.figure(figsize=(10, 15))
    ax1 = plt.subplot(len(features), 2, i)
    sns.distplot(df[df['Malware']==1][feature], ax=ax1, kde_kws={'bw': 0.1})
    ax1.set_title(f'Malware', fontsize=10)
    ax2 = plt.subplot(len(features), 2, i+1)
    sns.distplot(df[df['Malware']==0][feature], ax=ax2, kde_kws={'bw': 0.1})
    ax2.set_title(f'Not Malware', fontsize=10)
    i = i + 2

#plt.show()

# Preparar Conjuntos de Entrenamiento y Pruebas
X = dropped_df
y = df['Malware']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("Number of used features:", X_train.shape[1])

# Inicializar el Clasificador de Bosque Aleatorio
clf = RandomForestClassifier(
    n_estimators=100,  # Número de árboles
    random_state=0,    # Semilla para reproducibilidad
    oob_score=True,    # Calcular puntuación OOB
    max_depth=16       # Profundidad máxima de los árboles
)

# Ajustar el clasificador a los datos de entrenamiento
clf.fit(X_train, y_train)

# Predecir en el conjunto de prueba
y_pred = clf.predict(X_test)

# Imprimir Informe de Clasificación
print(classification_report(y_test, y_pred, target_names=['Not Malware', 'Malware']))

# Generar y Visualizar Matriz de Confusión
matrix = confusion_matrix(y_test, y_pred)
ax = sns.heatmap(matrix, annot=True, fmt="d", cmap='Blues', cbar=False, xticklabels=['Not Malware', 'Malware'], yticklabels=['Not Malware', 'Malware'])
ax.set_xlabel('Predicted Labels')
ax.set_ylabel('True Labels')

# Obtener la Importancia de las Características
importance = clf.feature_importances_
importance_dict = dict(zip(dropped_df.columns.values, importance))
sorted_importance = dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))

# Visualizar la Importancia de las Características
plt.figure(figsize=(10, 20))
sns.barplot(x=list(sorted_importance.values()), y=list(sorted_importance.keys()), palette='mako')
plt.xlabel('Importance Value')
plt.ylabel('Feature Name')
plt.title('Feature Importance in Random Forest Classifier')

#Guardar modelo
# Ajustar el clasificador a los datos de entrenamiento
clf.fit(X_train, y_train)

# Guardar modelo en un archivo pickle
with open('modelo_IA.pkl', 'wb') as model_file:
    pickle.dump(clf, model_file)
